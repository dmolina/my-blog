#+HUGO_BASE_DIR: ../
#+EXPORT_DATE: 
#+OPTIONS:  ^:nil
#+HUGO_SECTION: post/
#+HUGO_AUTO_SET_LASTMOD: t
#+DATE: 2012-07-15

* Programming                                                  :@programming:

** Improving performance in Python                       :python:performance:
   :PROPERTIES:
   :EXPORT_FILE_NAME: improving_python1
   :EXPORT_DATE: 2012-07-15
   :END:

All the source code of this post is available at [[https://github.com/dmolina/pyreal][github]].

In the previous post, I recognized my predilection for Python. For me, it is a great language for create prototypes in
many areas. For my research work, I usually creates/designs algorithms for continuous optimization using 
[[http://en.wikipedia.org/wiki/Evolutionary_algorithm][evolutionary algorithms]]. For these algorithms, languages like C/C++ or Java are widely used, specially for its
good performance (to publish, it is usual to have to make many comparisons between algorithms, so the performance
could be critical. However, for testing new ideas, many authors uses other tools like Mathlab that reduces the 
developer time at the cost of a higher computing time. 

I agree that Mathlab is great for numerical algorithms, but I still prefer Python over Mathlab, because I'm more confortable
with it, and have many libraries, and it's more simpler to call code in other languages, written in C or Java. That allow us
to increase the performance, and I like to test how much it could be improved. 

Several months ago, I start writing my most succesful algorithm, [[http://sci2s.ugr.es/EAMHCO/#macmals][Memetic Algorithms based on LS Chaining]], in Python. I had several
doubts about the performance, so I start writing one element, an Steady-State Genetic Algorithm, in Python. 

*** Calling C/C++ code from python

  The first challenge I had to tackle was to allow my python program to use the same benchmark functions than other implementations, 
  [[http://sci2s.ugr.es/EAMHCO/#TestF][CEC'2005 benchmark]]. 
  This benchmark define the functions to optimize, thus its main funtionality is 
  evaluate my solutions, when each solution is a vector of real numbers, with a real fitness value. 
  The benchmark code was implemented (by its authors) in C/C++. So, my python code have to call C++ code. 

  For doing that, I used the library [[http://www.boost.org/doc/libs/1_50_0/libs/python/doc/index.html][boost::python]], that is, in my opinion, the simpler way to call C/C++ code, specially
  when we uses [[http://numpy.scipy.org/][numpy]] package. 

  In my case, it is very simple, because I need few functions:

  #+begin_src python
    #include <boost/python.hpp>
    #include <boost/python/numeric.hpp>
    #include <boost/python/list.hpp>
    #include <iostream>
    #include "cec2005/cec2005.h"
    #include "cec2005/srandom.h"
  
    using namespace boost::python;
  
    Random r(new SRandom(12345679));
  
    void set_function(int fun, int dim) {
        init_cec2005(&r, fun, dim);
    }
  
    double evalua(const numeric::array &el) {
       const tuple &shape = extract<tuple>(el.attr("shape")); 
       unsigned n = boost::python::extract<unsigned>(shape[0]);
       double *tmp = new double[n];
      for(unsigned int i = 0; i < n; i++)
        {
          tmp[i] = boost::python::extract<double>(el[i]);
        }
      double result = eval_cec2005(tmp, n);
      delete tmp;
      return result; 
    }
    ...
  
    BOOST_PYTHON_MODULE(libpycec2005)
    {
        using namespace boost::python;
        numeric::array::set_module_and_type( "numpy", "ndarray");
        def("config", &set_function);
        def("evaluate", &evalua);
        ...
    }
  #+end_src

  More info in the good [[http://www.boost.org/doc/libs/1_50_0/libs/python/doc/index.html][boost::python]] documentation.

  One we can call C/C++ code, we have implemented the algorithm in python code. 
  The test code was the following: 

  #+begin_src python
    from ssga import SSGA
    from readargs import ArgsCEC05
    import libpycec2005 as cec2005
    import numpy
  
    def check_dimension(option, opt, value):
        if value not in [2, 10, 30, 50]:
            raise OptionValueError(
                "option %s: invalid dimensionality value: %r" % (opt, value))
  
    def main():
        """
        Main program
        """
        args = ArgsCEC05()
  
        if  args.hasError:
            args.print_help_exit()
  
        fun = args.function
        dim = args.dimension
  
        print "Function: %d" %fun
        print "Dimension: %d" %dim
        cec2005.config(fun, dim)
        domain = cec2005.domain(fun)
        print "Domain: ", domain
        ea = SSGA(domain=domain, size=60, dim=dim, fitness=cec2005.evaluate)
  
        for x in xrange(25):
            ea.run(maxeval=dim*10000)
            [bestsol, bestfit] = ea.getBest()
            print "BestSol: ", bestsol
            print "BestFitness: %e" %bestfit
            ea.reset()
  
    if __name__ == "__main__":
        main()
  #+end_src

  This source code run the algorithm 25 times, and in each run the algorithm stops when they are created 10000*dim solutions. 
  These conditions are indicated in the [[http://sci2s.ugr.es/EAMHCO/Tech-Report-May-30-05.pdf][benchmark specification]]. The only parameter was the function (-f, used function 1 by
  default) and dimension (-d) from 10, 30, 50.

*** Profiling the computing time

  How much time it takes? I have changed xrange(25) for xrange(1) and we have run its current version.
  The final time was 7 minutes for dimension 10, and 21 minutes for dimension 30 (for only one function). 

  Because I like to make more interesting things,  that only waiting computing time, I use the profile, only
  one run for the function, to detect the functions/method more expensive in computing time.

  #+begin_src bash
  python -m cProfile runcec.py -f 1 -d 10
  #+end_src

  The output was the following: 

  #+begin_src bash
          2943600 function calls (2943531 primitive calls) in 31.031 seconds

     Ordered by: standard name

     ncalls  tottime  percall  cumtime  percall filename:lineno(function)
  ....
        1    0.001    0.001    0.126    0.126 ssga.py:1(<module>)
      99940    0.561    0.000   17.463    0.000 ssga.py:109(cross)
          1    0.000    0.000    0.000    0.000 ssga.py:123(reset)
          1    5.559    5.559   51.129   51.129 ssga.py:126(run)
          1    0.000    0.000    0.000    0.000 ssga.py:14(__init__)
          1    0.000    0.000    0.000    0.000 ssga.py:158(getBest)
          1    0.000    0.000    0.000    0.000 ssga.py:31(set_mutation_rate)
      99940    0.730    0.000    1.885    0.000 ssga.py:45(mutation)
      12438    0.286    0.000    0.758    0.000 ssga.py:50(mutationBGA)
          1    0.002    0.002    0.002    0.002 ssga.py:77(initPopulation)
     105883    1.101    0.000    5.604    0.000 ssga.py:89(updateWorst)
          1    0.000    0.000    0.000    0.000 ssga.py:9(SSGA)
      99940    1.049    0.000   20.617    0.000 ssga.py:97(getParents)
  ...

  #+end_src

  With the profile we can observe the most expensive methods in our code:
  getParents (20 seconds), crossover operator (17 seconds), and updateWorst (5 seconds). 
  These methods are the 85% of the computing time, and the first two methods the 74% 
  of the computing time. 


*** Optimising the code

  That proves the majority of computing time is due to a minority of the code,
  only three methods. If we can optimize these methods, our code could be
  improved a lot.

  We can uses again the [[http://www.boost.org/doc/libs/1_50_0/libs/python/doc/index.html][boost::python]] package, but it's a bit tedious to use it. So, we have
  used the [[http://www.cython.org/][cython]] package. With cython we can optimize the source code adding
  information about the types.

  For instead, Instead of the following code:

  #+begin_src python
    import numpy as np
  
    def distance(ind1,ind2):
        """
        Euclidean distance
        ind1 -- first array to compare
        ind2 -- second array to compare
     
        Return euclidean distance between the individuals
  
        >>> from numpy.random import rand
        >>> import numpy as np
        >>> dim = 30
        >>> sol = rand(dim)
        >>> distance(sol,sol)
        0.0
        >>> ref=np.zeros(dim)
        >>> dist=distance(sol,ref)
        >>> dist > 0
        True
        >>> dist2 = distance(sol*2,ref)
        >>> 2*dist == dist2
        True
        """
        dif = ind1-ind2
        sum = (dif*dif).sum()
        return math.sqrt(sum)
  #+end_src

  we can write:

  #+begin_src python 
    cimport numpy as np
    cimport cython
    DTYPE = np.double
    ctypedef np.double_t DTYPE_t
    ctypedef np.int_t BTYPE_t
  
    def distance(np.ndarray[DTYPE_t, ndim=1]ind1, np.ndarray[DTYPE_t, ndim=1] ind2):
        """
        Euclidean distance
        ind1 -- first array to compare
        ind2 -- second array to compare
   
        ....  
        """
        cdef np.ndarray[DTYPE_t, ndim=1] dif = ind1-ind2
        cdef double sum = (dif*dif).sum()
        return math.sqrt(sum)
  #+end_src

  We can see that is still very readable. we only have put information about the type
  and dimension in the vector parameters and about the variables, using the keyword
  cdef.

  Let's see as an example the first method, the crossover operator, implemented
  in the crossBLX method:

  #+begin_src python
    import numpy as np
    import math
  
    def crossBLX(mother,parent,domain,alpha):
        """
        crossover operator BLX-alpha
      
        mother -- mother (first individual)
        parent -- parent (second individual)
        domain -- domain to check
        alpha  -- parameter alpha
  
        Returns the new children following the expression children = random(x-alpha*dif, y+alpha*dif), 
                    where dif=abs(x,y) and x=lower(mother,parents), y=upper(mother,parents) 
  
        >>> import numpy as np
        >>> low=-5
        >>> upper = 5
        >>> dim=30
        >>> sol = np.array([1,2,3,2,1])
        >>> crossBLX(sol,sol,[low,upper],0)
        array([ 1.,  2.,  3.,  2.,  1.])
        """
        diff = abs(mother-parent)
        dim = mother.size
        I=diff*alpha
        points = np.array([mother,parent])
        A=np.amin(points,axis=0)-I
        B=np.amax(points,axis=0)+I
        children = np.random.uniform(A,B,dim)
        [low,high]=domain
        return np.clip(children, low, high)
  
  #+end_src
 
  We can see that it is very simple to implement using numpy, but it is still very slow. With cython I have
  defined directly implement the many operations, the following code:

  #+begin_src python
    def crossBLX(np.ndarray[DTYPE_t, ndim=1] mother,np.ndarray[DTYPE_t, ndim=1] parent,list domain, double alpha):
        """
        ...
        """
        cdef np.ndarray[DTYPE_t, ndim=1] C, r
        cdef int low, high, dim
        cdef double x, y
        cdef double I, A, B
        cdef unsigned i
        [low,high]=domain
        dim = mother.shape[0]
        C = np.zeros(dim)
        r = random.randreal(0,1,dim)
  
        for i in range(dim):
            if mother[i] < parent[i]:
               (x,y) = (mother[i],parent[i])
            else:
               (y,x) = (mother[i],parent[i])
  
            I = alpha*(y-x)
            A=x-I
            B=y+I
          
            if (A < low):
                A = low
            if (B > high):
                B = high
          
            C[i] = A+r[i]*(B-A)
      
        return C
  
  #+end_src

  It's true that the source code is more complicated, but it is still very readable. 
  I have compared the two implementation to make sure both return the same values. 

*** Measuring the improvement

  How much these small changes in the code? 
  I have profile the source code again and it gives me:

  #+begin_src bash
           1020045 function calls (1019976 primitive calls) in 18.003 seconds

     Ordered by: standard name

     ncalls  tottime  percall  cumtime  percall filename:lineno(function)
  ....
          1    0.001    0.001    0.127    0.127 ssga.py:1(<module>)
      99940    0.425    0.000    2.432    0.000 ssga.py:109(cross)
          1    0.000    0.000    0.000    0.000 ssga.py:123(reset)
          1    5.415    5.415   17.864   17.864 ssga.py:126(run)
          1    0.000    0.000    0.000    0.000 ssga.py:14(__init__)
          1    0.000    0.000    0.000    0.000 ssga.py:158(getBest)
          1    0.000    0.000    0.000    0.000 ssga.py:31(set_mutation_rate)
      99940    0.699    0.000    2.006    0.000 ssga.py:45(mutation)
      12544    0.338    0.000    0.929    0.000 ssga.py:50(mutationBGA)
          1    0.002    0.002    0.002    0.002 ssga.py:77(initPopulation)
     105959    0.775    0.000    1.343    0.000 ssga.py:89(updateWorst)
          1    0.000    0.000    0.000    0.000 ssga.py:9(SSGA)
      99940    0.940    0.000    6.665    0.000 ssga.py:97(getParents)
  ....

  #+end_src

  We can see the improvement obtained. 

  |------------------+--------+--------|
  | Method           | Python | Cython |
  |------------------+--------+--------|
  | cross          : |   17.4 |    2.4 |
  | getParents     : |   20.6 |    6.6 |
  | updateWorst    : |    5.6 |    1.3 |
  |------------------+--------+--------|
  | Total            |   43.6 |   10.3 |
  |------------------+--------+--------|


  The new code takes only a 23% of the computing time of the previous code.
  With these changes, we have reduced the total time from 51 seconds to 18 code. 

*** In perspective

  Now, I run the source code without the profile, and test the source code obtaining the
  following time:

  |-------------+--------+-------------+--------|
  | Method      | dim=10 | dim=30      | dim=50 |
  |-------------+--------+-------------+--------|
  | Python      | 44s    | 3240s (54m) | --     |
  | Cython      | 10s    | 28s         | 48s    |
  |-------------+--------+-------------+--------|
  | Improvement | 77%    | 99%         | ---    |
  |-------------+--------+-------------+--------|

  In the following table, we test the maximum time for one and 25 runs (the time depends on the
  function used).

  |------------+---------+---------+--------|
  | #functions | dim=10  | dim=30  | dim=50 |
  |------------+---------+---------+--------|
  |          1 | 10s/18s | 28s/40s | 48s/1m |
  |         25 | 3/7m    | 15/21m  | 38m/   |
  |------------+---------+---------+--------|

  So, the total computing time is 7 minutes for dimension 10, and 21 minutes for dimension 30. 
  These numbers are very acceptable, specially because we can test in parallel the different functions 
  in a cluster of computers. Unfortunately, an implementation in Mathlab not only take more time, but
  also, for licensing reasons, it could not run in parallel without limit. 

  In resume, we can uses python code, not only to create experimental prototypes, but also to create 
  functional prototypes. 

  And about the possible testing problem, I've been working on it, but I think it is enough for a post, 
  didn't it? :-)

  All the code refered in the post, both in python and cython, is available at [[https://github.com/dmolina/pyreal][github]], if you want to see it. 

**** Fuente de la Tabla 1                                          :noexport:
  |------------------+--------+--------|
  | Method           | Python | Cython |
  |------------------+--------+--------|
  | cross          : |   17.4 |    2.4 |
  | getParents     : |   20.6 |    6.6 |
  | updateWorst    : |    5.6 |    1.3 |
  |------------------+--------+--------|
  | Total            |   43.6 |   10.3 |
  | ^                |    sum |    sum |
  |------------------+--------+--------|
  #+TBLFM: $sum=vsum(@2..@-1)

​* Footnotes
​* COMMENT Local Variables                                           :ARCHIVE:
# Local Variables:
# eval: (add-hook 'after-save-hook #'org-hugo-export-subtree-to-md-after-save :append :local)
# End:


** Callback that stop algorithm in R                                 :R:util:
   :PROPERTIES:
   :EXPORT_FILE_NAME: rmain
   :EXPORT_DATE: 2012-07-10
   :END:

Today I was making a little programming using the mathematical software R (very useful
 for statistics, by the way), for a little test. 

I'm one of the authors of a Cran package ([[http://cran.r-project.org/web/packages/Rmalschains/index.html][Rmalschains]]) for continuous optimization, and I was testing another packages to compare results. 

Comparing a particular package I realise that the API doesn't give me enough control for
the comparisons. Briefly, to compare different algorithms all of them should stop when the same
number of solutions is achieved. Unfortunately, for the DE package, the stopping criterion is the 
maximum iterations number, and for one strategy (the default strategy) this number differs, 
maintaining the same maximum iterations number, in function of the function to improve. I know, not 
so briefly :-).  

In resume, I want to pass a function to evaluate solutions to an algorithm, and that only the first
/maxEvals/ solutions could be considered. So, it should be nice that after /maxEvals/ function evaluations
the algorithm will stop. 

The aim is very simple in a theorical way, but I have only the control over a callback function used by
the algorithm, and I cannot use an 'exit' function into the function, because in that case will stop the global program, 
not only the current state of the algorithm. 

The solution? Using these 'complex' concepts that many people think that are useless, specially my CS students :-).
Combining a call with continuation with a closure:

#+begin_src R
finalFitness = callCC (function(exitFitness) {
     fitnessCheck <- function(fn, maxevals) {
          function(x) {

               if (total == 0 || total < maxevals) {
                  total <<- total +1;
                  fitness = fn(x);

                  if (total == 1 || fitness < bestFitness) {
                     bestFitness <<- fitness;
                  }    
                                          
               }
        
               if (total >= maxevals) {
                  exitFitness(bestFitness);
               }

                                        
               fitness;
           }

      }


      fitCheck = fitnessCheck(fun$fitness, fun$maxevals)

      log <- capture.output({
          total <- 0
          result=DEoptim(fitCheck, lower, upper, control=list(itermax=fun$maxevals/NP))
      })

      exitFitness(result$optim$bestval)
})
#+end_src

I know, it is a bit confusing. callCC implement the concept of /call-with-current-continuation/
to run a code with an /exit/ function *exitFitness* that allows me to stop the run of the algorithm. 
Because the function only does a run of the  algorithm (*DEOptim*), I can stop when I want. 
Also, to make it more elegant, I use a closure *fitnessCheck*  that receives a function and a 
maximum number of call, and it stops when the maximum calls number is achieved 
(/total/ and /bestFitness/ are global variable, so the way to modify their values is using
<<- instead of the classical <- or =). 

By the way, *capture.output* is a function that disables all the output of DEoptim algorithm. 



 
** DONE Property-Based Testing :python:testing:
   CLOSED: [2017-10-24 Tue 17:37]
   :PROPERTIES:
   :EXPORT_FILE_NAME: hypothesis_test
   :EXPORT_DATE: 2017-10-24
   :END:

Today I was reviewing a paper that I am doing in collaboration with other colleagues, 
a Phd student living in Sweden (in Västerås, a lovely city at a hour from Stockholm, 
I had a stay for 3 months three years ago). Then, to be sure that the proposed algorithm, 
a memetic version of a Differential Evolution, I started to implement it.  

During the code, I need to create group of random variables, without repetition.

it could be generated in the following way:

#+BEGIN_SRC python
for i in range(popsize):
    r1[i] = np.random.rand...
    r2[i] = np.random.rand...

    while r1[i] == r2[i]:
        r2[i] = np.random.rand...
#+END_SRC

However, it is not practical, for Matlab/Numpy it is better to work as vectors:

#+BEGIN_SRC python
r1 = np.random.choice(popsize, popsize)
r2 = np.random.choice(popsize, popsize)
# Avoid repeated values
r2 = change_repeated(r2, r1, popsize)
#+END_SRC

I need *change_repeated* to randomly generated again the values in r2 when it is equals 
than r1 (in the same position).

Then, I create the function:

#+BEGIN_SRC python
def change_repeated(values, original, maxvalue: int):
    """
    Repeat the values which are equals than original (position by position)

    :param values: array vector
    :param original: array vector or elements to not repeat
    :param maxvalue: maximum value (new ones will be between [0,  maxvalue])
    :returns: list of new values
    :rtype: ndarray.

    """
    equals, = np.nonzero(values == original)

    while len(equals) > 0:
        size = len(equals)
        values[equals] = np.random.choice(maxvalue, size)
        equals, = np.nonzero(values == original)

    return values
#+END_SRC

Great! Now, how I could test it? It could give several examples, and test again them. However, 
this type of tests can be very boring, so I applied [[https://es.slideshare.net/ScottWlaschin/an-introduction-to-property-based-testing][Property-based testing]].  

In this type of tests, instead of comparing examples of inputs and outputs, you test properties
that the function should follow for every possible input. 

For instance, if you have implemented sqrt function:

- Traditional testing :: Check sqrt(25)==5,  sqrt(16)==4,  sqrt(9)==3,  sqrt(5)=2.23....
 
- Property-based testing :: Check that the result of sqrt multiply for itself gives the original number.

#+BEGIN_EXPORT latex
sqrt(num)^2 == num,  \forall num \in \Real
sqrt(num) >= 0
#+END_EXPORT 

When you are using property-based testing, you can use a particular library to
automatically run your test with random inputs (with some constraints) hundreds
of times.
There are many tools for this: from the original [[https://en.wikipedia.org/wiki/QuickCheck][QuickCheck]], [[https://www.scalacheck.org/][ScalaCheck]] for Scala, or [[https://github.com/HypothesisWorks/hypothesis-python][Hypothesis]] for Python. 
In our case,  we are using the simple Hypothesis, that it is very [[https://hypothesis.readthedocs.io/en/latest/quickstart.html][well-documented]]. 

In our case, the properties of repeated(ys, xs) are:

- when xs and ys have some variable in common:

  + repeated(ys,  xs) must be different than ys.

  + the result must be different than original vector (to avoid to modify the original one).

  + The indexes where ys and repeated(ys, xs) are different xs == ys.

The testing was done in the following way:

First the import: 

#+BEGIN_SRC python
from hypothesis import given
import hypothesis.strategies as st
import numpy as np
#+END_SRC

- Then, we describe the range of float values:

#+BEGIN_SRC python
type_index = st.integers(min_value=0, max_value=popsize)
#+END_SRC

In that way, type_index create random values between [0,  popsize-1]

Then, we define the input as a list of previous integers:

#+BEGIN_SRC python 
type_list = st.lists(type_index, min_size=popsize, max_size=popsize)
#+END_SRC

We have set the min_size and max_size equals to get lists with the same size.
Hypothesis works by default with lists of different sizes.

Then, we say the type of each parameter:

#+BEGIN_SRC python
@given(type_list, type_list)
def test_random_norepeat(x, y):
    xs = np.array(x)
    ys_orig = np.array(y)
    ys = np.copy(ys_orig)

    if np.any(xs == ys):
        ys = change_repeated(ys, xs, dim)
        # It cannot be any repeated element
        assert not np.any(xs == ys)
        # It can be change any  algorithm
        assert not np.all(ys == ys_orig)
        # The change must be justify
        assert np.all((ys == ys_orig) | (ys_orig == xs))
        # Check that all values are between [0, dim]
        assert np.all((ys >= 0) & (ys < dim))
#+END_SRC

When it is run with *py.test* the hypothesis library will test the function with
hundreds of random xs and ys, when xs and ys are lists of size *popsize* and the
values between [0, popsize). If it fails, it will show you the /xs/ and /ys/
values for which the code fails the test, it is very useful.  

By the way, the original DE can be implemented like:

#+BEGIN_SRC python
    # Init population
    pop = pop_init(popsize, dim, min=min_value, max=max_value)
    fit = np.array([eval(sol) for sol in pop])
    nevals = 0

    while nevals <= maxevals:
        # Generate all random positions
        r1 = np.random.choice(popsize, popsize)
        r2 = np.random.choice(popsize, popsize)
        r3 = np.random.choice(popsize, popsize)
        # Avoid repeated values
        r2 = change_repeated(r2, r1, popsize)
        r3 = change_repeated_list(r3, [r1, r2], popsize)
        # New population, mutation
        V = pop[r1] + F*(pop[r2]-pop[r3])
        # Clipping
        V = np.clip(V, min_value, max_value)
        # Define U
        U = np.copy(pop)
        # Select cr
        cr = np.random.rand(popsize*dim).reshape((popsize, dim))
        # Make sure that for each individual a position is changed
        cr[np.arange(popsize), np.random.choice(dim, popsize)] = 0
        # Eq. 2 (crossover)
        U.flat[cr.flat < CR] = V.flat
        fitU = [eval(sol) for sol in U]
        nevals += popsize
        # Replacement
        better_cond = fitU < fit
        # Expand for dimension
        better_cond = np.repeat(better_cond, dim).reshape((popsize, dim))
        # Replace the best individual in population
        pop = np.where(better_cond, U, pop) 
        fit = np.minimum(fit, fitU)
#+END_SRC

I have to organize it a little, but in only 35 lines (including comments) a complete 
[[https://en.wikipedia.org/wiki/Differential_evolution][Differential Evolution]] can be implemented. Numpy is awesome!

* Emacs                                                              :@emacs:

** DONE Elfeed: Using emacs for reading RSS                           :emacs:
   CLOSED: [2017-10-12 Thu 17:53]
   :PROPERTIES:
   :EXPORT_FILE_NAME: emacs_rss
   :ID:       e208e2ea-28e5-4828-9042-a2d48542d3b3
   :END:

In last years I have been using Emacs for almost all my daily tasks: 

- Reading my emails (using [[http://www.djcbsoftware.nl/code/mu/mu4e.html][mu4e]]).
- Creating the slides for my courses using org-beamer.
- Using dired to navigate for the file system).
- Publishing this blog (using [[https://gohugo.io/][Hugo]] and [[https://ox-hugo.scripter.co][ox-hugo]]).

The last thing to integrate into emacs is reading blogs and news from RSS files. 
Adding [[https://github.com/skeeto/elfeed][elfeed]] and [[https://github.com/remyhonig/elfeed-org][elfeed-org]] I was able to create RSS. elfeed-org 
is very simple, it allows to add the feeds as items in org-mode:

#+BEGIN_EXAMPLE
- Blogs                                                              :elfeed:

  - https://www.meneame.net/rss                                  :news:portada:
  - https://www.meneame.net/rss?status=queued                            :news:
  - http://planet.emacsen.org/atom.xml                                :emacs:
  - https://www.reddit.com/r/programming/.rss                     :programming:
  ...
#+END_EXAMPLE
  
The tags for each feed will be shared for all articles. 

Then, loading *elfeed*, it can be obtained a screen showing the different articles:

[[/screen/elfeed.png]]

And selecting an article, it can be open, read and open each link by the default browser.

[[/screen/elfeed2.png]]

Several opinions about elfeed:

- It is very simple to use. 

- The use of tags is very powerful, not only they received the tags from the
  category, and you can add a tag to an article.

- The search filter is simple and very powerful, you can filter both for date and for tags. 

- The search filter can be kept as bookmark, so using C-x r b it can be seen the
  article using a particular filter.

To summary, *elfeed* has been a great discovery. If you use emacs, give it a try.

** DONE Fill-more or the important of reading documentation     :emacs:trick:
   CLOSED: [2017-12-15 Fri 11:28]
   :PROPERTIES:
   :EXPORT_FILE_NAME: emacs_justify
   :END:

I *love* Emacs and the auto-fill more. When I work I use it always to make
easier to read the text (with a small value, like 80 or 100). Then, if I have 
to copy to a Word Document (in collaboration with other people) or a text (like
in the submission of a review) I simple set the fill-column to a large value
(2000 or similar), with C-x f. Later, I copy all the text. 

Until now I have suffered in silence a small problem in text-mode (not in
org-mode). If you put

#+BEGIN_SRC sh
Text. 

- Item 1. 
- Item 2.
#+END_SRC  

After the fill-mode, you have:

#+BEGIN_SRC sh
Text. 

- Item 1 Item 2.
#+END_SRC

And to have in right you have to put a line between them:

#+BEGIN_SRC sh
Text. 

- Item 1.

- Item 2.
#+END_SRC

(The line between Text and first item is also required).

I though it was something inevitable, but checking the documentation, 

https://www.emacswiki.org/emacs/FillParagraph

I have known that with a simple line in elisp that behavior is fixed:

#+BEGIN_SRC elisp
    ;; The original value is "\f\\|[      ]*$", so we add the bullets (-), (+), and (*).
    ;; There is no need for "^" as the regexp is matched at the beginning of line.
    (setq paragraph-start "\f\\|[ \t]*$\\|[ \t]*[-+*] ")
#+END_SRC

I must check the available documentation more often :-).

* Teaching                                           :@teaching:@programming:

** DONE Using Python for Business Intelligence              :python:teaching:
   CLOSED: [2017-10-09 Mon 18:18]
   :PROPERTIES:
   :EXPORT_FILE_NAME: python_bi
   :EXPORT_DATE: 2017-10-09
   :EXPORT_HUGO_TAGS: teaching python
   :END:

Two weeks ago I started my first teaching day, replacing a teacher that has
still not gone to Ceuta (because the temporal contract was offered to many
people, and all of them refuse it). Do not worry, they will have the material, I
said to myself, naïvely. 

However, my Phd. advisor, the coordinator of the course, has decided to replace
the practice classes from [[https://www.knime.com/][Knime]] to Python using the different tools availables.
The reason? Because the Python, with R, are very popular in [[https://en.wikipedia.org/wiki/Data_science][Data Science]]. Also,
in Python there are very good tools for data analysis (like [[http://www.numpy.org/][numpy]], [[http://pandas.pydata.org/][pandas]]) or 
machine learning ([[http://scikit-learn.org/stable/][scikit-learn]], ...). It seems a good idea, but I have not
material, and I have only two days :-O.

Even more, I had still no access to the Moodle for the material of the course.
So, after a very busy Saturday, I finished including a material,  in  
http://github.com/dmolina/es_intro_python, with interesting references and an
install introduction.

Also, I use a very curious tool, https://gitpitch.com, that allow to create
slides from a markdown file from the repository github, [[https://gitpitch.com/dmolina/es_intro_python/master?grs=github&t=moon][Slides using Pitch]].

My final experience was:

- Very few students, so it was very relaxed because you can solve the problems
  for each student. However, using the [[https://www.anaconda.com/][anaconda]] there is few problems (and the .
  In prevision of the big size of the anaconda distribution, my downloaded
  version was copied by USB Disk to students.

- The [[http://jupyter.org/][jupyter notebook]] allow to test the python code without installing or
  learning an IDE (later they can install which they prefer, but for teaching
  you do not need any of them).

- You have to prepare exercises, because if not, you talk and show and you can
  finished in few minutes a material that takes you many hours.

- When you have only a weekend for preparing material, I must have already strong
  knowledge about the topic (fortunately,  it was my case). If not, you will not
  be confident teaching it.  

For the second day, I was preparing another slide for teaching pandas (with the
most useful operations, by my experience), available as pdf format here:
[slides_pandas.pdf](./slides/slides_pandas.pdf) (In a future post, I will say as I
prepare my slides using Emacs+Org-mode). /Unfortunately/, the new teacher was
ready,  and I have to finish my courses using python for BI. 
